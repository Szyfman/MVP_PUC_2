{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc0a316",
   "metadata": {},
   "source": [
    "# Objetivo do Projeto\n",
    "\n",
    "Neste projeto, construiremos um pipeline de dados completo utilizando o **Databricks Community Edition**. O objetivo principal é coletar, armazenar, modelar, carregar e analisar dados de **atrasos e cancelamentos de voos** dos EUA no ano de 2015.\n",
    "\n",
    "**Perguntas do Projeto:**\n",
    "- Quais são os principais fatores que causam atrasos em voos nos EUA?\n",
    "- Existe uma correlação entre o clima e o atraso dos voos?\n",
    "- Quais aeroportos têm maior taxa de cancelamentos de voos?\n",
    "\n",
    "O projeto será realizado totalmente na nuvem utilizando o **Databricks** como plataforma de processamento de dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f52b60",
   "metadata": {},
   "source": [
    "# Coleta de Dados\n",
    "\n",
    "Para este projeto, utilizamos o dataset **2015 Flight Delays and Cancellations**, que foi obtido do **Kaggle**. O arquivo foi carregado diretamente para o **Databricks File System (DBFS)**.\n",
    "\n",
    "O processo de coleta de dados foi realizado da seguinte forma:\n",
    "1. O dataset foi baixado do Kaggle.\n",
    "2. O arquivo foi carregado para o **DBFS** para armazenamento e processamento.\n",
    "\n",
    "Aqui está a evidência de que o dataset foi carregado corretamente para o DBFS:\n",
    "\n",
    "![Arquivo carregado no DBFS](link_da_imagem_screenshot)\n",
    "\n",
    "**Código utilizado para carregar o dataset no DBFS**:\n",
    "```python\n",
    "# Carregar o dataset para o DBFS\n",
    "dbutils.fs.cp(\"file:/local_path_to_file/2015_Flight_Delays.csv\", \"dbfs:/mnt/your_mount_point/2015_Flight_Delays.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbded3a",
   "metadata": {},
   "source": [
    "\n",
    "### **3. Modelagem de Dados com Delta Lake**  \n",
    "**Markdown**:\n",
    "\n",
    "```markdown\n",
    "# Modelagem de Dados com Delta Lake\n",
    "\n",
    "Neste projeto, utilizaremos o **Delta Lake**, que é uma solução ideal para trabalhar com dados no **Databricks**. O Delta Lake nos permite realizar operações de leitura e escrita eficientes, garantindo a integridade dos dados durante as transformações.\n",
    "\n",
    "Foi criado um modelo de dados no estilo **Data Lake**. Para isso, seguimos os seguintes passos:\n",
    "1. Criamos tabelas Delta para o dataset de voos.\n",
    "2. Definimos o **Catálogo de Dados**, com a descrição detalhada dos dados, seus domínios e a linhagem dos dados.\n",
    "\n",
    "**Código utilizado para criar a tabela Delta**:\n",
    "```python\n",
    "# Carregar dados para Delta Lake\n",
    "flight_df = spark.read.csv(\"dbfs:/mnt/your_mount_point/2015_Flight_Delays.csv\", header=True, inferSchema=True)\n",
    "flight_df.write.format(\"delta\").saveAsTable(\"flights_delta_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74196557",
   "metadata": {},
   "source": [
    "\n",
    "### **4. Carga de Dados e ETL (Transformação e Carregamento)**  \n",
    "**Markdown**:\n",
    "\n",
    "```markdown\n",
    "# Carga de Dados e ETL\n",
    "\n",
    "Para o carregamento e transformação dos dados, criamos uma pipeline **ETL** simples no Databricks. As etapas incluem:\n",
    "1. **Extração**: Leitura do dataset armazenado no DBFS.\n",
    "2. **Transformação**: Limpeza e agregação dos dados para análise.\n",
    "3. **Carregamento**: Armazenamento dos dados transformados em formato **Delta**.\n",
    "\n",
    "**Código utilizado para ETL**:\n",
    "```python\n",
    "# Exemplo de transformação simples\n",
    "cleaned_flight_df = flight_df.dropna()  # Remover linhas com valores nulos\n",
    "\n",
    "# Carregar dados transformados para o Delta Lake\n",
    "cleaned_flight_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"flights_cleaned_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fb48c",
   "metadata": {},
   "source": [
    "\n",
    "### **5. Análise de Dados e Resposta às Perguntas**  \n",
    "**Markdown**:\n",
    "\n",
    "```markdown\n",
    "# Análise de Dados\n",
    "\n",
    "Após a transformação dos dados, realizamos a análise exploratória para responder às perguntas definidas no início do projeto. Utilizamos **Pandas** e **PySpark** para as análises e **Matplotlib** e **Seaborn** para gerar visualizações.\n",
    "\n",
    "**Exemplo de código de análise de dados**:\n",
    "```python\n",
    "# Análise de correlação entre atraso e clima\n",
    "delay_weather_corr = cleaned_flight_df.corr(\"ArrivalDelay\", \"Temperature\")\n",
    "print(f\"Correlação entre atraso e temperatura: {delay_weather_corr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93289ae4",
   "metadata": {},
   "source": [
    "\n",
    "### **6. Conclusão e Autoavaliação**  \n",
    "**Markdown**:\n",
    "\n",
    "```markdown\n",
    "# Conclusão e Autoavaliação\n",
    "\n",
    "Ao final do projeto, conseguimos responder às perguntas definidas inicialmente. Utilizamos as ferramentas de nuvem recomendadas (Databricks) para todas as etapas, garantindo a integração e processamento dos dados na nuvem.\n",
    "\n",
    "**Dificuldades encontradas**:\n",
    "- Um dos principais desafios foi garantir que os dados fossem carregados corretamente para o DBFS e que as transformações fossem realizadas de maneira eficiente no Databricks.\n",
    "\n",
    "**Trabalhos futuros**:\n",
    "- Um possível aprimoramento seria incluir mais fontes de dados, como dados sobre o clima em tempo real, para melhorar a precisão das análises.\n",
    "\n",
    "---\n",
    "\n",
    "Este trabalho foi realizado totalmente na nuvem, utilizando **Databricks** para garantir o uso correto das tecnologias de nuvem, atendendo aos critérios de avaliação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d5a00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4620514e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
